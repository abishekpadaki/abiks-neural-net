{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building a simple auto-encoder\n",
        "\n",
        "In this notebook I'm trying to create a simple neural nets that act as an auto-encoder - compressing data like images and then decompressing them at inference."
      ],
      "metadata": {
        "id": "WotBxDmGi-71"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2ExsVMMQioMA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data prep work\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaFFl39hi8ux",
        "outputId": "b512e2bb-0e0b-4dfb-8d2e-652a965d7e0a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 16.1MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 491kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.49MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.29MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model prep\n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "\n",
        "  def __init__(self) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = nn.Sequential(\n",
        "        nn.Linear(28*28, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128,32)\n",
        "    )\n",
        "\n",
        "    self.decoder = nn.Sequential(\n",
        "        nn.Linear(32, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, 28*28),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = x.view(x.size(0),-1) #reshape the 4d tensor mnsit input to flat vector for encoder\n",
        "\n",
        "    #The .view() function is PyTorch’s way to reshape tensors.\n",
        "    #x.size(0) is the batch size — here, 64. So this line says:\n",
        "    #“Keep the batch size dimension the same, but flatten everything else into one long vector.”\n",
        "\n",
        "    #That -1 is a neat PyTorch trick telling it to figure out the size automatically\n",
        "    #for that dimension based on the total number of elements and other given dimensions.\n",
        "\n",
        "    z = self.encoder(x)\n",
        "    out = self.decoder(z)\n",
        "    out = out.view(x.size(0),1,28,28) #batch, channels, height, width\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "ml2iJX8AjxYl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoEncoder()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr=0.001)"
      ],
      "metadata": {
        "id": "_EqQsS1BlZjp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train loop\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  running_loss = 0.0\n",
        "\n",
        "  for imgs, _ in train_loader:\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(imgs)\n",
        "    loss = criterion(outputs,imgs)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss+= loss.item()\n",
        "\n",
        "  avg_loss = running_loss / len(train_loader)\n",
        "\n",
        "  print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCHa6vbxmYKz",
        "outputId": "3225ee60-8673-42b3-bb5c-c44a7d28544d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 0.0366\n",
            "Epoch 2/5, Loss: 0.0156\n",
            "Epoch 3/5, Loss: 0.0117\n",
            "Epoch 4/5, Loss: 0.0101\n",
            "Epoch 5/5, Loss: 0.0091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some quick notes:\n",
        "\n",
        "- __Why ReLU:__ ReLU is the go-to activation because it’s simple, fast, and helps the network learn complex stuff without suffering from vanishing gradients like sigmoid or tanh can. It basically says, “If the input is positive, keep it; if not, zero it out.” This sparsifies activations and speeds up training, which is handy in those hidden layers.\n",
        "\n",
        "- __Why sigmoid at the end of decoder:__ The sigmoid at the end is because MNIST images are normalized between 0 and 1 (grayscale pixel intensities). Sigmoid squashes outputs to exactly that range, so the decoder’s output can be compared directly to the original input pixels. Without it, the network might output weird values outside that range, messing up the reconstruction loss.\n",
        "\n",
        "- __Why Adam:__ Adam optimizer is like the Swiss Army knife of optimizers — it adapts learning rates on the fly for each parameter, combining the benefits of momentum and RMSProp. This makes training faster and more stable, especially for smaller networks like our autoencoder. Plus, it’s just widely used and reliable in practice.\n",
        "\n",
        "- The MNSIT dataset has a 4D Tensor where the 4 dimensions are:\n",
        "(__batch_size__, __channels__, __height__, __width__)"
      ],
      "metadata": {
        "id": "NBf9cgQ-oDse"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0W690C7gozIp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}