{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11442010",
   "metadata": {},
   "source": [
    "# Autograd\n",
    "Automatic gradient calculations for optimization of nn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab9f589d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.8570, -0.4600, -1.6681], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "328cce80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1430, 1.5400, 0.3319], grad_fn=<AddBackward0>)\n",
      "tensor(2.5254, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x+2\n",
    "#this creates a computational graph with + op as a node, x an 2 as inputs to the node and y as output\n",
    "#autograd then creates a gradient for y wrt x (dy/dx) in x \n",
    "\n",
    "print(y)\n",
    "\n",
    "z = y*y*2\n",
    "z= z.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394c46c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5239, 2.0533, 0.4426])\n",
      "tensor([-0.8570, -0.4600, -1.6681], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#Call the gradients\n",
    "z.backward() #dz/dx\n",
    "\n",
    "#usually the last value before backward needs to be a scalar, else throws an error\n",
    "#this will not work if x(the leaf) has requires_grad=False\n",
    "print(x.grad)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c28dce",
   "metadata": {},
   "source": [
    "## Stopping or not adding a tensor to the autograd graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3165ac5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4585,  0.8202,  0.8619], requires_grad=True)\n",
      "tensor([-0.4585,  0.8202,  0.8619])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y = x.detach()\n",
    "print(y)\n",
    "#you can also use x.reuqires_grad_(False)\n",
    "#or\n",
    "#x.detach()\n",
    "#or\n",
    "# with torch.no_grad():"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a5217f",
   "metadata": {},
   "source": [
    "## Accessing gradients and avoiding gradient accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b218155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([6., 6., 6., 6.])\n",
      "tensor([9., 9., 9., 9.])\n"
     ]
    }
   ],
   "source": [
    "#Lets take a dummy training loop example\n",
    "\n",
    "weights = torch.ones(4,requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model_output = (weights*3).sum()\n",
    "    model_output.backward()\n",
    "\n",
    "    print(weights.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9cb2d1",
   "metadata": {},
   "source": [
    "As you can see, the gradients are being accumulated each time the loop runs\n",
    "i.e, the gradients keep getting added to weights each time backward is run.\n",
    "\n",
    "To prevent this, we always set the tensor to grad.zero_() after calling backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a180ae9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "#same as before but with grad_zero_()\n",
    "weights = torch.ones(4,requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model_output = (weights*3).sum()\n",
    "    model_output.backward()\n",
    "\n",
    "    print(weights.grad)\n",
    "    weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d527f0f8",
   "metadata": {},
   "source": [
    "And this is how you would do it when working with optimizers (like SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae7ca038",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.ones(4,requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.SGD([weights], lr=0.01)\n",
    "optimizer.step() #run the optimizer step\n",
    "optimizer.zero_grad() #same as grad.zero()_ but for optimizers\n",
    "\n",
    "#grad.zero()_ is used only for a single variable/tensor.\n",
    "#when using optimizer and otherwise, always use _zero.grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bd35f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.10.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
