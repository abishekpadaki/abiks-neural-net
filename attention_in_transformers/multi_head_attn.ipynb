{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ry8OQDUjZzCq"
      },
      "outputs": [],
      "source": [
        "import torch ## torch let's us create tensors and also provides helper functions\n",
        "import torch.nn as nn ## torch.nn gives us nn.module() and nn.Linear()\n",
        "import torch.nn.functional as F # This gives us the softmax()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using same attention template class as previous notebooks.\n",
        "## NOTE: This below is Encoder-Decoder Self Attention\n",
        "\n",
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model=2,\n",
        "                 row_dim=0,\n",
        "                 col_dim=1):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "\n",
        "        self.row_dim = row_dim\n",
        "        self.col_dim = col_dim\n",
        "\n",
        "\n",
        "    ## The only change from SelfAttention and attention is that\n",
        "    ## now we expect 3 sets of encodings to be passed in...\n",
        "    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None):\n",
        "        ## ...and we pass those sets of encodings to the various weight matrices.\n",
        "        q = self.W_q(encodings_for_q)\n",
        "        k = self.W_k(encodings_for_k)\n",
        "        v = self.W_v(encodings_for_v)\n",
        "\n",
        "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
        "\n",
        "        scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5)\n",
        "\n",
        "        if mask is not None:\n",
        "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
        "\n",
        "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
        "\n",
        "        attention_scores = torch.matmul(attention_percents, v)\n",
        "\n",
        "        return attention_scores\n"
      ],
      "metadata": {
        "id": "RiJ0pexuauBt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a class for multihead attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model=2, row_dim=0, col_dim=1, num_heads=1):\n",
        "\n",
        "    super().__init__()\n",
        "    self.row_dim = row_dim\n",
        "    self.col_dim = col_dim\n",
        "\n",
        "    # create a few attention heads\n",
        "\n",
        "    self.heads = nn.ModuleList([\n",
        "        Attention(d_model,row_dim,col_dim) for _ in range(num_heads)]\n",
        "    )\n",
        "\n",
        "  def forward(self, encodings_for_q, encodings_for_k, encodings_for_v):\n",
        "\n",
        "    #run data through all attention heads\n",
        "\n",
        "    return torch.cat([head(encodings_for_q,encodings_for_k,encodings_for_v) for head in self.heads], dim=self.col_dim)"
      ],
      "metadata": {
        "id": "wVclampwZ6Y4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## set the seed for the random number generator\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FljmfD0jbVX_",
        "outputId": "0a6ec3c4-2857-4559-fcbe-2e658a24819f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f2af5985790>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## create an attention object\n",
        "multiHeadAttention = MultiHeadAttention(d_model=2,\n",
        "                                        row_dim=0,\n",
        "                                        col_dim=1,\n",
        "                                        num_heads=1)"
      ],
      "metadata": {
        "id": "l7NJ6gJMbbX0"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## create matrices of token encodings...\n",
        "encodings_for_q = torch.tensor([[1.16, 0.23],\n",
        "                                [0.57, 1.36],\n",
        "                                [4.41, -2.16]])\n",
        "\n",
        "encodings_for_k = torch.tensor([[1.16, 0.23],\n",
        "                                [0.57, 1.36],\n",
        "                                [4.41, -2.16]])\n",
        "\n",
        "encodings_for_v = torch.tensor([[1.16, 0.23],\n",
        "                                [0.57, 1.36],\n",
        "                                [4.41, -2.16]])"
      ],
      "metadata": {
        "id": "Y9g9qlN3bndy"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## calculate regular encoder-decoder attention (i.e 1 head)\n",
        "multiHeadAttention(encodings_for_q, encodings_for_k, encodings_for_v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9_anv60bdxS",
        "outputId": "25979f17-d53e-4912-bd71-77cb888ff7fa"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0100, 1.0641],\n",
              "        [0.2040, 0.7057],\n",
              "        [3.4989, 2.2427]], grad_fn=<CatBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now for attn with multiple heads\n",
        "\n",
        "## create an attention object\n",
        "multiHeadAttention = MultiHeadAttention(d_model=2,\n",
        "                                        row_dim=0,\n",
        "                                        col_dim=1,\n",
        "                                        num_heads=2)\n",
        "\n",
        "## calculate encoder-decoder attention with 2 heads\n",
        "multiHeadAttention(encodings_for_q, encodings_for_k, encodings_for_v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWMw4ZOibhED",
        "outputId": "9557c0e8-aa20-4fbd-a964-938d4ff5790f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.7081, -0.8268,  0.6226,  0.1312],\n",
              "        [-0.7417, -0.9193,  0.5522,  0.2499],\n",
              "        [-0.7190, -0.8447,  0.5669,  0.2324]], grad_fn=<CatBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we get 4 attention values (2heads * 2 values)"
      ],
      "metadata": {
        "id": "hhU9Quf7cLT-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zvpSvND7cJ4B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}