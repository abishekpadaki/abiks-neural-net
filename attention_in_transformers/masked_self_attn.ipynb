{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0Ef75YQV-KjM"
      },
      "outputs": [],
      "source": [
        "import torch ## torch let's us create tensors and also provides helper functions\n",
        "import torch.nn as nn ## torch.nn gives us nn.module() and nn.Linear()\n",
        "import torch.nn.functional as F # This gives us the softmax()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model=2, row_dim=0, col_dim=1):\n",
        "\n",
        "      super().__init__()\n",
        "\n",
        "      self.W_q = nn.Linear(in_features=d_model,out_features=d_model, bias=False)\n",
        "      self.W_k = nn.Linear(in_features=d_model,out_features=d_model, bias=False)\n",
        "      self.W_v = nn.Linear(in_features=d_model,out_features=d_model, bias=False)\n",
        "\n",
        "      self.row_dim = row_dim\n",
        "      self.col_dim = col_dim\n",
        "\n",
        "    def forward(self, token_encodings, mask=None):\n",
        "\n",
        "      q = self.W_q(token_encodings)\n",
        "      k = self.W_k(token_encodings)\n",
        "      v = self.W_v(token_encodings)\n",
        "\n",
        "      sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
        "\n",
        "      scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5)\n",
        "\n",
        "      if mask is not None:\n",
        "        scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
        "\n",
        "      attn_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
        "\n",
        "      attn_scores = torch.matmul(attn_percents, v)\n",
        "\n",
        "      return attn_scores\n"
      ],
      "metadata": {
        "id": "BUl8iBZ1-cal"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## create a matrix of token encodings...\n",
        "encodings_matrix = torch.tensor([[1.16, 0.23],\n",
        "                                 [0.57, 1.36],\n",
        "                                 [4.41, -2.16]])"
      ],
      "metadata": {
        "id": "44YzUhy2AdKu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1neTh6riAn77",
        "outputId": "c95b6ab2-1eab-47dd-ef27-3a663b648b42"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e1e7932d910>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## create a masked self-attention object\n",
        "maskedSelfAttention = MaskedSelfAttention(d_model=2,\n",
        "                               row_dim=0,\n",
        "                               col_dim=1)"
      ],
      "metadata": {
        "id": "Fo_kwhi2AqKd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask = torch.tril(torch.ones(3, 3))\n",
        "\n",
        "\n",
        "#Tril converts 1s to 0s in the upper triangle\n",
        "\n",
        "# 111\n",
        "# 111\n",
        "# 111\n",
        "\n",
        "# To\n",
        "\n",
        "# 100\n",
        "# 110\n",
        "# 111"
      ],
      "metadata": {
        "id": "gKaFnlgLAs2A"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask = mask == 0 # convert 0 to True and 1 to False\n",
        "mask # print out the mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqOv6aatAwZ0",
        "outputId": "271eb7bf-2bd6-4bfc-eff2-183ff7e8a0ae"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[False,  True,  True],\n",
              "        [False, False,  True],\n",
              "        [False, False, False]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maskedSelfAttention(encodings_matrix, mask) #Get masked self attn values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDXldUrIAyZb",
        "outputId": "02ac5eec-bcc3-486f-84e4-e0ceb89eebd5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.6038,  0.7434],\n",
              "        [-0.0062,  0.6072],\n",
              "        [ 3.4989,  2.2427]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wwIBiE2mBppG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}